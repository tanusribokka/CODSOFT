{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install zipfile36"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CIJOhmg4WlK",
        "outputId": "3bdc3919-ae7b-45d0-df89-80e777d8723b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zipfile36\n",
            "  Downloading zipfile36-0.1.3-py3-none-any.whl.metadata (736 bytes)\n",
            "Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/spam.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pol9prlf4ZPh",
        "outputId": "89ab2a25-0b08-497c-ac01-9f57109a606e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/spam.zip, /content/spam.zip.zip or /content/spam.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('spam.csv',encoding='Windows-1252')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "data['v1'] = data['v1'].astype(str)\n",
        "data.dropna(subset=['v1'], inplace=True)\n",
        "data['v1'] = data['v1'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "9O9x4uZO9AtT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "data['v1'] = label_encoder.fit_transform(data['v1'])\n",
        "X = data['v2']\n",
        "y = data['v1']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n"
      ],
      "metadata": {
        "id": "7F3grYTb-XPB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "nb_predictions = nb_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "fpuGdEP5BMZ_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "lr_predictions = lr_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "CSgKvUH6BnMg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_predictions = svm_model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "HWvgMg8cBpHd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(predictions, model_name):\n",
        "    print(f\"{model_name} Model Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, predictions):.2f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, predictions):.2f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, predictions):.2f}\")\n",
        "    print()\n",
        "evaluate_model(nb_predictions, \"Naive Bayes\")\n",
        "evaluate_model(lr_predictions, \"Logistic Regression\")\n",
        "evaluate_model(svm_predictions, \"SVM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyf2YahGDGAZ",
        "outputId": "6418c97b-ac3a-4b2c-b98a-e1e873f0a5b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Performance:\n",
            "Accuracy: 0.97\n",
            "Precision: 1.00\n",
            "Recall: 0.75\n",
            "F1 Score: 0.86\n",
            "\n",
            "Logistic Regression Model Performance:\n",
            "Accuracy: 0.97\n",
            "Precision: 0.99\n",
            "Recall: 0.77\n",
            "F1 Score: 0.87\n",
            "\n",
            "SVM Model Performance:\n",
            "Accuracy: 0.98\n",
            "Precision: 0.99\n",
            "Recall: 0.89\n",
            "F1 Score: 0.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Assuming you have a trained model (e.g., svm_model) and TF-IDF vectorizer (tfidf)\n",
        "joblib.dump(svm_model, 'sms_spam_classifier_model.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efutcHSKBdUQ",
        "outputId": "84a34abf-b63e-4811-a742-271be8c0823d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import joblib\n",
        "\n",
        "# Load the trained model and TF-IDF vectorizer\n",
        "model = joblib.load('sms_spam_classifier_model.pkl')\n",
        "tfidf = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# Preprocess function (same as used for training)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "def predict_sms(text):\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    text_tfidf = tfidf.transform([preprocessed_text])\n",
        "    prediction = model.predict(text_tfidf)\n",
        "    if prediction[0] == 1:\n",
        "        return \"Spam\"\n",
        "    else:\n",
        "        return \"Ham\"\n"
      ],
      "metadata": {
        "id": "EseX9i7DBzux"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_sms = input(\"Enter the SMS text: \")\n",
        "result = predict_sms(user_sms)\n",
        "print(f\"The SMS is classified as: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKgk-hd3B14M",
        "outputId": "b6046ad3-0468-4c23-b0ef-84716a6c817c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the SMS text: Even my brother is not like to speak with me. They treat me like aids patent.\n",
            "The SMS is classified as: Ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "Uy0XTsJcD5OG"
      }
    }
  ]
}